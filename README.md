# PySpark Learning Journey with Databricks ğŸš€

Welcome to my repository where I document my learning and hands-on practice with **PySpark** on **Databricks**. This journey covers everything from the basics to advanced data engineering and big data concepts. I used this project to deepen my understanding of distributed computing, real-world data transformations, and optimization techniques within the Databricks ecosystem.

## ğŸ“Œ About This Repository

This repository includes:
- Code notebooks developed and tested on **Databricks**.
- End-to-end PySpark examples for working with structured and semi-structured data.
- Exploration of optimization, job orchestration, and integrations with cloud-based storage.

Whether you're a beginner or someone looking to sharpen your PySpark skills, this repository aims to be a comprehensive reference for working with big data using PySpark on Databricks.

---

## ğŸ“š Topics Covered (More notebooks to be added soon as I am working on them) 

### ğŸ”° Basics
- Introduction to PySpark and Spark architecture
- Creating RDDs and DataFrames
- Basic transformations and actions
- Reading and writing data (CSV, JSON, Parquet)

### ğŸ§ª DataFrames & Transformations
- Schema definition and inference
- Filtering, grouping, aggregation
- Window functions and ranking
- UDFs and performance considerations

### ğŸ—ï¸ Data Engineering Workflows
- Reading from and writing to **Delta Lake**
- Handling **partitioning** and **bucketing**
- Working with large-scale datasets from **Azure Blob Storage** or **GCP Buckets**
- Data cleaning and null handling techniques

### ğŸ› ï¸ Advanced Topics
- **Joins** (inner, outer, semi, anti)
- **Broadcast joins** and optimization
- **Caching** and **persistence** strategies
- **Spark SQL and temporary views**
- **Structured Streaming** with PySpark
- Integration with **Apache Kafka**

### ğŸ§¹ Performance Tuning
- Understanding **Spark DAGs**
- **Shuffle optimization**
- **Skew handling**
- Partition pruning
- Job and stage analysis using the **Spark UI**

### ğŸ§© Extras
- Job orchestration using **Apache Airflow**
- Monitoring with **Databricks job clusters**
- Deploying jobs via **Databricks Jobs UI**

---

## ğŸ§‘â€ğŸ’» Tools & Technologies

- **PySpark**
- **Databricks Community Edition**
- **Delta Lake**
- **Apache Airflow** (for orchestration demos)
- **Cloud Storage**: GCP & Azure
- **Jupyter Notebook** (for local testing)

---

## ğŸ“ Notes

Each notebook is self-contained and includes explanations, comments, and sample outputs. Datasets used for demos are either publicly available or synthetically generated.

> This project is a personal learning initiative and not affiliated with any organization. Contributions, suggestions, and feedback are welcome!

---

## ğŸ“« Connect with Me

Feel free to reach out if you're working on similar projects or want to discuss PySpark, data engineering, or cloud workflows.

- LinkedIn: [(https://www.linkedin.com/in/hrishikeshgr/)]

---

â­ï¸ **If you find this repo helpful, consider giving it a star!**
